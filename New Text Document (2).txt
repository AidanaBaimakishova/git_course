hello world!

# Работа с массивами данных
import numpy as np

# Работа с таблицами
import pandas as pd

# Отрисовка графиков
import matplotlib.pyplot as plt

# Функции-утилиты для работы с категориальными данными
from tensorflow.keras import utils

# Класс для конструирования последовательной модели нейронной сети
from tensorflow.keras.models import Sequential

# Основные слои
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation, LSTM

# Токенизатор для преобразование текстов в последовательности
from tensorflow.keras.preprocessing.text import Tokenizer

# Заполнение последовательностей до определенной длины
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Загрузка датасетов из облака google
import gdown

# Для работы с файлами в Colaboratory
import os

# Отрисовка графиков
import matplotlib.pyplot as plt

%matplotlib inline

gdown.download('https://storage.yandexcloud.net/aiueducation/Content/base/l7/tesla.zip', None, quiet=True)

# Распаковка архива в папку writers
!unzip -qo tesla.zip -d tesla/

# Просмотр содержимого папки
!ls tesla

# Объявляем функции для чтения файла. На вход отправляем путь к файлу
def read_text(file_name):

  # Задаем открытие нужного файла в режиме чтения
  read_file = open(file_name, 'r')

  # Читаем текст
  text = read_file.read()

  # Переносы строки переводим в пробелы
  text = text.replace("\n", " ")

  # Возвращаем текст файла
  return text

# Объявляем интересующие нас классы
class_names = ["Негативный отзыв", "Позитивный отзыв"]

# Считаем количество классов
num_classes = len(class_names)


import os
# Создаём список под тексты для обучающей выборки
texts_list = []

# Циклом проводим итерацию по текстовым файлам в папке отзывов
for j in os.listdir('/content/tesla/'):

  # Добавляем каждый файл в общий список для выборки
        texts_list.append(read_text('/content/tesla/' + j))

        # Выводим на экран сообщение о добавлении файла
        print(j, 'добавлен в обучающую выборку')

# Узнаем объём каждого текста в словах и символах
texts_len = [len(text) for text in texts_list]

# Устанавливаем "счётчик" номера текста
t_num = 0

# Выводим на экран  информационное сообщение
print(f'Размеры текстов по порядку (в токенах):')

# Циклом проводим итерацию по списку с объёмами текстов
for text_len in texts_len:

  # Запускаем "счётчик" номера текста
  t_num += 1

  # Выводим на экран сообщение о номере и объёме текста
  print(f'Текст №{t_num}: {text_len}')

# Создаём список с вложенным циклом по длинам текстов, где i - 100% текста, i/5 - 20% текста
train_len_shares = [(i - round(i/5)) for i in texts_len]

# Устанавливаем "счётчик" номера текста
t_num = 0

# Циклом проводим итерацию по списку с объёмами текстов равными 80% от исходных
for train_len_share in train_len_shares:

  # Запускаем "счётчик" номера текста
  t_num += 1

  # Выводим на экран сообщение о номере и объёме текста в 80% от исходного
  print(f'Доля 80% от текста №{t_num}: {train_len_share} символов')

from itertools import chain
# Функция для нарезки текста по заданной доле (в процентах)
def split_text(text, share_percent):
    share_length = round(len(text) * share_percent)
    return text[:share_length]

# Задаем долю для обучающей выборки (80%)
train_share_percent = 0.8

# Используем функцию chain() для объединения текстов в один список
all_texts = list(chain(*texts_list))

# Нарезаем общий текст по соответствующей доле
train_length = round(len(all_texts) * train_share_percent)

# Нарезаем общий текст на обучающую и проверочную выборки
train_text = all_texts[:train_length]
val_text = all_texts[train_length:]

# Выводим информацию о размерах обучающего и проверочного текстов
print(f'Размер обучающего текста (в токенах): {len(train_text)}')
print(f'Размер проверочного текста (в токенах): {len(val_text)}')

all_labels = [0] * len(train_text) + [1] * len(val_text)

# Преобразуем метки в формат one-hot encoding
y_train = utils.to_categorical(all_labels[:len(train_text)], num_classes)
y_val = utils.to_categorical(all_labels[len(train_text):], num_classes)

# Инициализируем токенизатор
max_words = 10000
tokenizer = Tokenizer(num_words=max_words, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', oov_token='неизвестное_слово')

# Подготавливаем токенизатор на обучающих данных
tokenizer.fit_on_texts(train_text)

# Преобразуем тексты в последовательности индексов
x_train_sequences = tokenizer.texts_to_sequences(train_text)
x_val_sequences = tokenizer.texts_to_sequences(val_text)

# Максимальная длина последовательности (в токенах)
max_len = 200

# Заполняем (паддим) последовательности до нужной длины
x_train_pad = pad_sequences(x_train_sequences, maxlen=max_len)
x_val_pad = pad_sequences(x_val_sequences, maxlen=max_len)

# Выводим информацию о размерах обучающей и проверочной выборок
print(f'Размер x_train: {x_train_pad.shape}')
print(f'Размер y_train: {y_train.shape}')
print(f'Размер x_val: {x_val_pad.shape}')
print(f'Размер y_val: {y_val.shape}')


MAX_WORDS_COUNT=max_len
WIN_SIZE=200

# Архитектура со слоем Embedding и регуляризацией
model = Sequential()
model.add(Embedding(input_dim = MAX_WORDS_COUNT, output_dim = 20, input_length=WIN_SIZE))
model.add(SpatialDropout1D(0.2))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(200, activation="relu"))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Обучение модели
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=128, validation_data=(x_val_pad, y_val), verbose=2)